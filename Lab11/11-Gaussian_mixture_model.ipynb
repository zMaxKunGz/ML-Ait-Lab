{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Gaussian Mixture Models (GMMs)\n",
    "\n",
    "Today we continue with unsupervised learning. One model that's more sophisticated than $k$-means is the Gaussian Mixture Model (GMM).\n",
    "\n",
    "The GMM models a dataset $(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(m)})$ as an i.i.d. sample from the following\n",
    "generative model for each sample\n",
    "$\\mathbf{x}^{(i)}$:\n",
    "\n",
    "1. Sample $z^{(i)}$ from a multinomial distribution over clusters $1..k$ according to probabilities $(\\phi_1,\\ldots,\\phi_k)$.\n",
    "2. Sample $\\mathbf{x}^{(i)}$ from ${\\cal N}(\\mathbf{\\mu}_{z^{(i)}},\\Sigma_{z^{(i)}})$.\n",
    "\n",
    "The parameters are estimated using the Expectation Maximization (EM) algorithm, which begins with a guess for parameters\n",
    "$\\phi_1,\\ldots,\\phi_k,\\mu_1,\\ldots,\\mu_k,\\Sigma_1,\\ldots,\\Sigma_k$ then iteratively alternates between computing a soft assignment\n",
    "of data to clusters then updating the parameters according to that soft assignment.\n",
    "\n",
    "First, we'll build a GMM model for a dataset then use the model for anomaly detection.\n",
    "\n",
    "## Example 1: Anomaly detection\n",
    "\n",
    "Let's generate synthetic data from a mixture of Gaussians, use EM to recover as best possible the ground truth parameters, and\n",
    "then use the model to find \"anomalies\" (unusually unlikely points according to the model). First, we set up the ground truth\n",
    "parameters and generate a dataset from those ground truth parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ground truth means and covariances for the data we'll generate\n",
    "\n",
    "means_gt = [ [1,10], [10,1], [10,10] ]\n",
    "sigmas_gt = [ np.matrix([[1, 0],[0, 1]]), np.matrix([[4,0],[0,1]]),\n",
    "              np.matrix([[1,0],[0,4]]) ]\n",
    "\n",
    "# Ground truth Prior probability (phi_j) for each cluster\n",
    "\n",
    "phi_gt = [ 0.2, 0.2, 0.6 ]\n",
    "\n",
    "# For more interesting covariances, you can also try, for example,\n",
    "# [[11.31371, -0.70711],[11.31371, 0.70711]] or\n",
    "# [[11.31371, 0.70711],[-11.31371, 0.70711]].\n",
    "\n",
    "# Size of dataset\n",
    "\n",
    "m = 500\n",
    "\n",
    "# number of variables\n",
    "\n",
    "n = len(means_gt[0])\n",
    "\n",
    "# k number of clusters/outcomes\n",
    "\n",
    "k = len(phi_gt)\n",
    "\n",
    "# Ground truth indices of cluster identities\n",
    "\n",
    "Z = [0]*m\n",
    "\n",
    "# Generate a new k-means dataset\n",
    "\n",
    "def gen_dataset():\n",
    "    X = np.zeros((m,n))\n",
    "    # Generate m samples from multinomial distribution using phi_gt\n",
    "    z_vectors = np.random.multinomial(1, phi_gt, size=m)  # Result: binary matrix of size (m x k)\n",
    "    for i in range(m):\n",
    "        # Convert one-hot representation z_vectors[i,:] to an index\n",
    "        Z[i] = np.where(z_vectors[i,:] == 1)[0][0]\n",
    "        # Grab ground truth mean mu_{z^i}\n",
    "        mu = means_gt[Z[i]]                \n",
    "        # Grab ground truth covariance Sigma_{z^i}\n",
    "        sigma = sigmas_gt[Z[i]]\n",
    "        # Sample a 2D point from mu, sigma\n",
    "        X[i,:] = np.random.multivariate_normal(mu,sigma,1)\n",
    "    return X\n",
    "\n",
    "\n",
    "X = gen_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the EM algorithm itself. We have an initialization step and an iterative step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gmm(X, k):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    Mu = np.zeros((n,k))\n",
    "    Sigma = np.zeros((k,n,n))\n",
    "    Phi = np.zeros(k)\n",
    "    order = np.random.permutation(m)\n",
    "    for j in range(k):\n",
    "        # Initially assign equal probability to each cluster/outcome\n",
    "        Phi[j] = 1/k      \n",
    "        # Ramdomly assign mean to one of the data points       \n",
    "        Mu[:,j] = X[order[j],:].T\n",
    "        # Initial covariance is identity matrix\n",
    "        Sigma[j,:,:] = np.eye(n)    \n",
    "    return Phi, Mu, Sigma\n",
    "\n",
    "\n",
    "def Gaussian(X, mean, covariance):\n",
    "    k = len(mean)\n",
    "    X = X - mean.T\n",
    "    p = 1/((2*np.pi)**(k/2)*(np.linalg.det(covariance)**0.5)) * np.exp(-0.5 * np.sum(X @ np.linalg.pinv(covariance) * X, axis=1))\n",
    "    return p\n",
    "\n",
    "def gaussian(x, mean,covariance):\n",
    "    k = len(mean)\n",
    "    X = (x - mean).reshape(-1,1)\n",
    "    p = 1/((2*np.pi)**(k/2)*(np.linalg.det(covariance)**0.5)) * np.exp(-0.5 * (X.T @ np.linalg.pinv(covariance) @ X))\n",
    "    return p\n",
    "\n",
    "\n",
    "# Run one iteration of EM\n",
    "\n",
    "def iterate_em_gmm(X, threshold, Phi, Mu, Sigma):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    k = len(Phi)\n",
    "    threshold = np.reshape(np.repeat(threshold, n*k), (n,k))\n",
    "    pj_arr = np.zeros((m,k))\n",
    "    \n",
    "    # E-step: calculate w_j^i\n",
    "    W = np.zeros((m, k))\n",
    "    for j in range(k):\n",
    "        pj = Gaussian(X, Mu[:,j], Sigma[j])\n",
    "        pj_arr[:,j] = pj\n",
    "        W[:,j] = Phi[j] * pj\n",
    "    \n",
    "    # W tells us what is the relative weight of each cluster for each data point\n",
    "    W[:,:] = W * np.tile(1/np.sum(W,1),(k,1)).T\n",
    "\n",
    "    # M-step: adjust mean and sigma\n",
    "    Phi[:] = sum(W) / m\n",
    "    Mu_previous = Mu.copy()\n",
    "    for j in range(k):\n",
    "        # Split cluster specific W for each dimension\n",
    "        Wj = np.tile(W[:,j],(2,1)).T\n",
    "        # Compute Mu for each variable for each cluster\n",
    "        Mu[:,j] = sum(X * Wj)/sum(Wj)\n",
    "        Muj = np.tile(Mu[:,j],(m,1))\n",
    "        Sigma[j,:,:] = np.matmul((X - Muj).T, (X - Muj) * Wj) / sum(W[:,j])\n",
    "\n",
    "    if (abs(Mu-Mu_previous) <= threshold).all():\n",
    "        converged = True\n",
    "    else:\n",
    "        converged = False\n",
    "\n",
    "    labels = np.argmax(pj_arr, axis = 1)\n",
    "    pj = np.max(pj_arr,axis=1)\n",
    "    X_label = np.insert(X, 2, labels, axis=1)\n",
    "    return converged, pj, X_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model to convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.matrix(.01)\n",
    "\n",
    "Phi, Mu, Sigma = init_gmm(X, k)\n",
    "\n",
    "converged = False\n",
    "while not converged:\n",
    "    converged, pj, X_label = iterate_em_gmm(X, threshold, Phi, Mu, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Phi)\n",
    "print(phi_gt)\n",
    "\n",
    "phi_gt = np.array(phi_gt).reshape(-1,1)\n",
    "phi_mse = np.mean(np.min((Phi-phi_gt)**2,axis=1))\n",
    "\n",
    "print(phi_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Mu)\n",
    "print(np.array(means_gt).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sigma)\n",
    "print(sigmas_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise (not graded, for practice)\n",
    "\n",
    "Determine how close the estimated parmeters Phi, Mu, and Sigma are to the ground trouth values set up at the beginning of the experiment. Report your results and briefly discuss here.\n",
    "\n",
    "Next, we continue to find outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_prob = .01\n",
    "outliers = np.nonzero(pj < outlier_prob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15,10))\n",
    "\n",
    "xlist = np.linspace(-3, 20, 100)\n",
    "ylist = np.linspace(-3, 20, 100)\n",
    "XX, YY = np.meshgrid(xlist, ylist)\n",
    "ZZ = np.zeros(XX.shape)\n",
    "\n",
    "for c in np.arange(0,k):    \n",
    "    X_class = X[np.where(X_label[:,2] == c)[0],:]\n",
    "    \n",
    "    Z = np.zeros(XX.shape)\n",
    "    i = 0\n",
    "    while i < XX.shape[0]:\n",
    "        j = 0\n",
    "        while j < XX.shape[0]:\n",
    "            pt = np.array([[XX[i,j], YY[i,j]]])\n",
    "            Z[i,j] = Gaussian(pt, Mu[:,c], Sigma[c])[0]\n",
    "            j = j + 1\n",
    "        i = i + 1    \n",
    "    ZZ = np.maximum(ZZ,Z)\n",
    "cp = plt.contourf(XX, YY, ZZ,alpha=0.5)\n",
    "cbar = fig1.colorbar(cp)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],marker=\".\",c=pj,cmap='viridis');\n",
    "plt.scatter(X[outliers,0],X[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);\n",
    "plt.plot(Mu[0,0], Mu[1,0],'bx',Mu[0,1], Mu[1,1],'bx', Mu[0,2], Mu[1,2],'bx')\n",
    "\n",
    "plt.title('Inliers and outliers according to GMM model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise\n",
    "\n",
    "Notice that using a hard threshold for each cluster gives us more outliers for a broad cluster than a\n",
    "tight cluster. First, you'll want to understand why and explain.  Next, you'll read about\n",
    "Mahalanobis distance\n",
    "of a point to the mean of a multivariate Gaussian distribution and see if you can use Mahalanobis distance\n",
    "to get a better notion of outliers in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c46176d4de6de8615692f9812d326359",
     "grade": false,
     "grade_id": "cell-17581751276375e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.1 (10 points)\n",
    "\n",
    "Notice that using a hard threshold for each cluster gives us more outliers for a broad cluster than a tight cluster. Understand why, and explain here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need code to explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5a23af6e6cf72351b4fa7df2a400593",
     "grade": true,
     "grade_id": "cell-c7ab9cdb0f9bbb63",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbea76efb12cb2ee8607a4adb92b54f7",
     "grade": false,
     "grade_id": "cell-92f1c8427624ccab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2 (15 points)\n",
    "\n",
    "Read about Mahalanobis distance of a point to the mean of a multivariate Gaussian distribution and see if you can use Mahalanobis distance to get a better notion of outliers in this dataset.\n",
    "1. Explain what is Mahalanobis (5 points)\n",
    "2. Write code Mahalanobis (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb14a53aedc2931ef0ff8dd4eebead14",
     "grade": false,
     "grade_id": "cell-414861ea9c7221a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Explain what is Mahalanobis distance (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6b8325a7465f1030c7cfea1bd2d29cf",
     "grade": true,
     "grade_id": "cell-535c7333dd962e9e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d9ce2ade2596554fbafb16e472405de",
     "grade": false,
     "grade_id": "cell-84ac454737c4a7d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Write code to compute Mahalanobis distance between the data and their nearest means (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6395976c7ef4a7fb5132be481f0d243e",
     "grade": false,
     "grade_id": "cell-20d7898f6bfbbf31",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(Sigma.shape)\n",
    "print(Mu.shape)\n",
    "print(X.shape)\n",
    "\n",
    "m_distance = np.zeros((X.shape[0],Mu.shape[1]))\n",
    "\n",
    "for kk in range(Mu.shape[1]):\n",
    "    for i, x in enumerate(X):\n",
    "        # get all row data from target column\n",
    "        mu = None\n",
    "        # get target sigma\n",
    "        sig = None\n",
    "        # inverse matrix of sigma\n",
    "        sig_inv = None\n",
    "        # find difference of mu and x and reshape it (if need)\n",
    "        diff = None\n",
    "        # calculate distance from diff and sigma\n",
    "        distance = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # keep distance\n",
    "        m_distance[i,kk] = distance\n",
    "\n",
    "# find unique of minimum m_distance and count\n",
    "# hint: use np.unique and np.argmin\n",
    "(unique, counts) = None, None\n",
    "\n",
    "max_z_score = 2.05\n",
    "# find minimum distance\n",
    "# hint: use np.min\n",
    "min_distance = None\n",
    "# find outlier from min_distance\n",
    "outlier = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7b504092910a80aa0cb0782e04cbe85",
     "grade": true,
     "grade_id": "cell-569fec737414bfdb",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function: Do not remove\n",
    "print('outlier', outlier)\n",
    "\n",
    "fig1 = plt.figure(figsize=(15,10))\n",
    "\n",
    "xlist = np.linspace(-3, 20, 100)\n",
    "ylist = np.linspace(-3, 20, 100)\n",
    "XX, YY = np.meshgrid(xlist, ylist)\n",
    "ZZ = np.zeros(XX.shape)\n",
    "\n",
    "for c in np.arange(0,k):    \n",
    "    X_class = X[np.where(X_label[:,2] == c)[0],:]\n",
    "    \n",
    "    Z = np.zeros(XX.shape)\n",
    "    i = 0\n",
    "    while i < XX.shape[0]:\n",
    "        j = 0\n",
    "        while j < XX.shape[0]:\n",
    "            pt = np.array([[XX[i,j], YY[i,j]]])\n",
    "            Z[i,j] = Gaussian(pt, Mu[:,c], Sigma[c])[0]\n",
    "            j = j + 1\n",
    "        i = i + 1    \n",
    "    cp = plt.contour(XX,YY,Z)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],marker=\".\",c=pj,cmap='viridis');\n",
    "plt.scatter(X[outlier,0],X[outlier,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);\n",
    "plt.plot(Mu[0,0], Mu[1,0],'bx',Mu[0,1], Mu[1,1],'bx', Mu[0,2], Mu[1,2],'bx')\n",
    "\n",
    "plt.title('Inliers and outliers according to GMM model')\n",
    "plt.show()\n",
    "print('success!')\n",
    "# End test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Customer segmentation\n",
    "\n",
    "In this example we will use the Kaggle customer segmentation dataset\n",
    "from last week. We've included the\n",
    "[Mall_Customers.csv](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)\n",
    "file with this lab.\n",
    "\n",
    "Let's stick to just two dimensions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Mall_Customers.csv')\n",
    "data = data.drop(['CustomerID', 'Gender', 'Age'], axis = 1)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = np.array(data, dtype=float)\n",
    "\n",
    "n = X.shape[1]\n",
    "m = X.shape[0]\n",
    "k = 3\n",
    "\n",
    "threshold = np.matrix(.01)\n",
    "\n",
    "# Slightly different version of init_gmm due to the data format and spread\n",
    "def init_gmm(X, k):\n",
    "    Mu = np.zeros((n,k))\n",
    "    Sigma = np.zeros((k,n,n))\n",
    "    Phi = np.zeros(k)\n",
    "    order = np.random.permutation(m)\n",
    "    for j in range(k):\n",
    "        Phi[j] = 1/k\n",
    "        Mu[:,j] = X[order[j],:].T\n",
    "        Sigma[j,:,:] = np.cov(X.T)\n",
    "    return Phi, Mu, Sigma\n",
    "\n",
    "\n",
    "Phi, Mu, Sigma = init_gmm(X, k)\n",
    "\n",
    "converged = False\n",
    "while not converged:\n",
    "    converged, pj, X_label = iterate_em_gmm(X, threshold, Phi, Mu, Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row represents annual income, whereas the second row represents the spending score. These values tend to change in every iteration, so therefore it is difficult to definitively segregate the data into three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sigma.shape)\n",
    "print(Mu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[:,0],X[:,1],marker=\".\",c=pj,cmap='viridis');\n",
    "\n",
    "outlier_prob = .0002\n",
    "outliers = np.nonzero(pj<outlier_prob)[0]\n",
    "\n",
    "\n",
    "xlist = np.linspace(-20, 150, 100)\n",
    "ylist = np.linspace(-20, 120, 100)\n",
    "XX, YY = np.meshgrid(xlist, ylist)\n",
    "ZZ = np.zeros(XX.shape)\n",
    "for c in np.arange(0,k):    \n",
    "    X_class = X[np.where(X_label[:,2] == c)[0],:]\n",
    "    Z = np.zeros(XX.shape)\n",
    "    i = 0\n",
    "    while i < XX.shape[0]:\n",
    "        j = 0\n",
    "        while j < XX.shape[0]:\n",
    "            pt = np.array([[XX[i,j], YY[i,j]]])\n",
    "            Z[i,j] = Gaussian(pt, Mu[:,c], Sigma[c])\n",
    "            j = j + 1\n",
    "        i = i + 1    \n",
    "    ZZ = np.maximum(ZZ,Z)\n",
    "cp = plt.contourf(XX, YY, ZZ,alpha=0.5)\n",
    "plt.scatter(X[outliers,0],X[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);\n",
    "plt.plot(Mu[0,0], Mu[1,0],'bx',Mu[0,1], Mu[1,1],'bx', Mu[0,2], Mu[1,2],'bx')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a86fb34b591ca4b15807f054dead5f0",
     "grade": false,
     "grade_id": "cell-2394ab5307e1f3ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In-class exercise (25 points)\n",
    "\n",
    "Examine the cluster centers and determine whether you can find any reasonable interpretation of them.\n",
    "Discuss in your report (5 points), and compare to the $k$-means method from last week. (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de73ef3bbb4c481663ccffca922af174",
     "grade": false,
     "grade_id": "cell-96312501821d55f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Interpretation of clusters (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97fed3c156f1b450acf873f01dd38661",
     "grade": true,
     "grade_id": "cell-fc054b29f728345f",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef7505e3d5cdd241c5b8342af81c45e0",
     "grade": false,
     "grade_id": "cell-5dec8275511fae66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Run $k$-means and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c9838d797e157b6629e70450a9a7b91",
     "grade": true,
     "grade_id": "cell-38c9400e79e13094",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1793b5b237bc1e9d43017a6b0eb9c5bb",
     "grade": false,
     "grade_id": "cell-ee2ec130aa58107f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In-class exercise (10 points)\n",
    "\n",
    "Do the same analysis with Mahalanobis distance as in the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "607e767b82930e0804e7d60dc5cb7496",
     "grade": false,
     "grade_id": "cell-86bb5d28ea398e0c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "outlier = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "061de06bb7a2717622993c25c6ee1c89",
     "grade": true,
     "grade_id": "cell-66c39bb496e6240d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function: Do not remove\n",
    "print('outlier', outlier)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[:,0],X[:,1],marker=\".\",c=pj,cmap='viridis');\n",
    "\n",
    "plt.scatter(X[outlier,0],X[outlier,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);\n",
    "plt.plot(Mu[0,0], Mu[1,0],'bx',Mu[0,1], Mu[1,1],'bx', Mu[0,2], Mu[1,2],'bx')\n",
    "\n",
    "for c in np.arange(0,k):    \n",
    "    X_class = X[np.where(X_label[:,2] == c)[0],:]\n",
    "    xlist = np.linspace(0, 150, 50)\n",
    "    ylist = np.linspace(0, 120, 50)\n",
    "    \n",
    "    XX, YY = np.meshgrid(xlist, ylist)\n",
    "    Z = np.zeros(XX.shape)\n",
    "    i = 0\n",
    "    while i < XX.shape[0]:\n",
    "        j = 0\n",
    "        while j < XX.shape[0]:\n",
    "            pt = np.array([[XX[i,j], YY[i,j]]])\n",
    "            Z[i,j] = Gaussian(pt, Mu[:,c], Sigma[c])\n",
    "            j = j + 1\n",
    "        i = i + 1    \n",
    "    cp = plt.contour(XX, YY, Z)\n",
    "\n",
    "plt.show()\n",
    "print('success!')\n",
    "# End test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Customer segmentation\n",
    "\n",
    "This example is based on [Nguyen Hanh's tutorial on Medium.com](https://medium.com/@nguyenbaha/buiding-customer-segmentation-by-gmm-from-scratch-4ea6adc3da1c). In this example we use the Kaggle [OnlineRetail.csv](https://www.kaggle.com/vijayuv/onlineretail) dataset for customer segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Online_Retail.csv')\n",
    "data = data.iloc[0:5000,:]\n",
    "print(data.head())\n",
    "data = data.drop(['InvoiceNo','Description', 'CustomerID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the categorical and numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_colmns = data.select_dtypes(include=['object']).columns\n",
    "print(categorical_colmns)\n",
    "numerical_colmns = data._get_numeric_data().columns\n",
    "print(numerical_colmns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_percentage(data):\n",
    "    \"\"\"This function takes a DataFrame(df) as input and returns two columns, \n",
    "     total missing values and total missing values percentage\"\"\"\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = round(data.isnull().sum().sort_values(ascending = False)/len(data)*100,2)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n",
    "\n",
    "\n",
    "missing_percentage(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fill the \"na\" values with \"No information\" and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[categorical_colmns] = data[categorical_colmns].fillna(\"No information\")\n",
    "data[numerical_colmns] = data[numerical_colmns].fillna(0)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the categorical columns to numeric. Give some thought as to whether it's appropriate\n",
    "to do so for all of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_to_numeric(categorical_columns):\n",
    "    i = 0;\n",
    "    columnname = '';\n",
    "    while i < len(categorical_colmns):\n",
    "        col_idx = data.columns.get_loc(categorical_colmns[i])\n",
    "        distinct_values = data[categorical_colmns[i]].unique()\n",
    "        j = 0;    \n",
    "        for val in distinct_values:\n",
    "            idx = np.where(data[categorical_colmns[i]] == val);\n",
    "            data.iloc[idx[0],col_idx] = j            \n",
    "            j = j + 1;\n",
    "        i = i + 1;\n",
    "        \n",
    "        \n",
    "category_to_numeric(data[categorical_colmns])\n",
    "\n",
    "data = data.astype('float64')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mu = np.mean(data[numerical_colmns])\n",
    "Sigma = np.std(data[numerical_colmns])\n",
    "print(Mu)\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers\n",
    "\n",
    "def cnt_outlier(data, sigma, mu, inc_cols=[]):\n",
    "    num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    num_cols = [e for e in num_cols if e in inc_cols]\n",
    "    outlier = (data[numerical_colmns]-mu).abs() > sigma*3\n",
    "    return outlier.sum()\n",
    "\n",
    "cnt_outlier(data, Sigma, Mu, numerical_colmns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data[data.duplicated()]) > 0:\n",
    "    print(\"No. of duplicated entries: \", len(data[data.duplicated()]))\n",
    "    print(data[data.duplicated(keep=False)].sort_values(by=list(data.columns)).head())\n",
    "    data.drop_duplicates(inplace=True)\n",
    "else:\n",
    "    print(\"No duplicated entries found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b50394bd39db048250d4779b97a42a70",
     "grade": false,
     "grade_id": "cell-f44da3186cabaa21",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## In-class and take-home exercise\n",
    "\n",
    "Use the same GMM code as in the previous two examples on this dataset. Try to interepret the\n",
    "results you get and plot the inliers/outliers with a Mahalanobis distance\n",
    "threshold. Plot likelihood as a function of $k$ and determine whether there is an \"elbow\"\n",
    "in the plot. How many clusters should you use? Describe your experiments and results in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values\n",
    "\n",
    "mean = np.mean(X,axis=0)\n",
    "std = np.std(X,axis=0)\n",
    "\n",
    "X = (X-mean)/std\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
