{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Convolutional Neural Networks\n",
    "\n",
    "Today we'll experiment with CNNs. We'll start with a hand-coded CNN structure based on numpy, then we'll move to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-coded CNN\n",
    "\n",
    "This example is based on [Ahmed Gad's tutorial](https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html).\n",
    "\n",
    "We will implement a very simple CNN in numpy. The model will have\n",
    "just three layers, a convolutional layer (conv for short), a ReLU activation layer, and max pooling. The major steps involved are as follows.\n",
    "1. Reading input images.\n",
    "2. Preparing filters.\n",
    "3. Conv layer: Convolving each filter with the input image.\n",
    "4. ReLU layer: Applying ReLU activation function on the feature maps (output of conv layer).\n",
    "5. Max Pooling layer: Applying the pooling operation on the output of ReLU layer.\n",
    "6. Stacking the conv, ReLU, and max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading an input image\n",
    " \n",
    "The following code reads an existing image using the SciKit-Image Python library and converts it into grayscale. You may need to `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read image\n",
    "img = skimage.data.chelsea()\n",
    "print('Image dimensions:', img.shape)\n",
    "\n",
    "# Convert to grayscale\n",
    "img = skimage.color.rgb2gray(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some filters for the conv layer\n",
    "\n",
    "Recall that a conv layer uses some number of convolution (actually cross correlation) filters, usually matching\n",
    "the number of channels in the input (1 in our case since the image is grayscale). Each kernel gives us one\n",
    "feature map (channel) in the result.\n",
    "\n",
    "In this case, the convolution (cross correlation) computation can be written\n",
    "$$\\mathtt{X}'(i, j, k) = (\\mathtt{X} \\star \\mathtt{K}_k)(i, j) = \\sum_m \\sum_n \\sum_c \\mathtt{X}(i+m, j+n, c)\n",
    "\\mathtt{K}_k(m, n, c),$$\n",
    "where $\\mathtt{X}$ is an input tensor with three dimensions ($H$ rows, $W$ columns, and $C$ channels),\n",
    "$\\mathtt{K}_k$ is likewise an input tensor with three dimensions ($M$ rows, $N$ columns, and $C$ channels),\n",
    "resulting in feature map $\\mathtt{X}'(\\cdot, \\cdot, k)$, i.e., the $k$th channel of the overall output tensor.\n",
    "If our conv layer contains $K$ kernels $\\mathtt{K}_1, \\ldots, \\mathtt{K}_K$, after $K$ of these convolution\n",
    "operations, the resulting tensor $\\mathtt{X}'$ will contain $K$ channels.\n",
    "\n",
    "Let's make two $3\\times3\\times1$ filters, using the horizontal and vertical Sobel edge filters, flipped horizontally\n",
    "and vertically so that they give positive values for increasing intensity across an edge when used in cross\n",
    "correlation rather than convolution mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filters = np.zeros((2,3,3))\n",
    "l1_filters[0, :, :] = np.array([[[-1, 0, 1], \n",
    "                                 [-2, 0, 2], \n",
    "                                 [-1, 0, 1]]])\n",
    "l1_filters[1, :, :] = np.array([[[-1, -2, -1], \n",
    "                                 [ 0,  0,  0], \n",
    "                                 [ 1,  2,  1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv layer feedforward step\n",
    " \n",
    "Let's convolve (correlate) an input image with our filters. The convolution will be stride\n",
    "1 and valid region only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution (cross correlation operation) with stride 1 for a single channel kernel,\n",
    "# assuming the input image is also single channel.\n",
    "\n",
    "def convolve(img, conv_filter):\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    results_dim = ((np.array(img.shape) - np.array(conv_filter.shape) + (2*padding))/stride) + 1\n",
    "    result = np.zeros((int(results_dim[0]), int(results_dim[1])))\n",
    "    \n",
    "    for r in np.arange(0, img.shape[0] - filter_size + 1):\n",
    "        for c in np.arange(0, img.shape[1]-filter_size + 1):          \n",
    "            curr_region = img[r:r+filter_size,c:c+filter_size]\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result)\n",
    "            result[r, c] = conv_sum\n",
    "    \n",
    "    return result       \n",
    "\n",
    "# Perform convolution with a set of filters and return the result\n",
    "\n",
    "def conv(img, conv_filters):\n",
    "    # Check shape of inputs\n",
    "    if len(img.shape) != len(conv_filters.shape) - 1: \n",
    "        raise Exception(\"Error: Number of dimensions in conv filter and image do not match.\")  \n",
    "\n",
    "    # Ensure filter depth is equal to number of channels in input\n",
    "    if len(img.shape) > 2 or len(conv_filters.shape) > 3:\n",
    "        if img.shape[-1] != conv_filters.shape[-1]:\n",
    "            raise Exception(\"Error: Number of channels in both image and filter must match.\")\n",
    "            \n",
    "    # Ensure filters are square\n",
    "    if conv_filters.shape[1] != conv_filters.shape[2]: \n",
    "        raise Exception('Error: Filter must be square (number of rows and columns must match).')\n",
    "\n",
    "    # Ensure filter dimensions are odd\n",
    "    if conv_filters.shape[1]%2==0: \n",
    "        raise Exception('Error: Filter must have an odd size (number of rows and columns must be odd).')\n",
    "\n",
    "    # Prepare output\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filters.shape[1]+1, \n",
    "                             img.shape[1]-conv_filters.shape[1]+1, \n",
    "                             conv_filters.shape[0]))\n",
    "\n",
    "    # Perform convolutions\n",
    "    for filter_num in range(conv_filters.shape[0]):\n",
    "        curr_filter = conv_filters[filter_num, :]\n",
    "        # Our convolve function only handles 2D convolutions. If the input has multiple channels, we\n",
    "        # perform the 2D convolutions for each input channel separately then add them. If the input\n",
    "        # has just a single channel, we do the convolution directly.\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = convolve(img[:, :, 0], curr_filter[:, :, 0])\n",
    "            for ch_num in range(1, curr_filter.shape[-1]):\n",
    "                conv_map = conv_map + convolve(img[:, :, ch_num], \n",
    "                                      curr_filter[:, :, ch_num])\n",
    "        else:\n",
    "            conv_map = convolve(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = conv(img, l1_filters)\n",
    "%timeit conv(img,l1_filters)\n",
    "\n",
    "print('Convolutional feature maps shape:', features.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(features[:,:,0], cmap='gray')\n",
    "ax2.imshow(features[:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, right? A couple observations:\n",
    "1. We've hard coded the values in the filters, so they are sensible to us. In a real CNN, we'd be tuning the filters to minimize loss on the training set, so we wouldn't expect such perfectly structured results.\n",
    "2. Naive implementation of 2D convolutions requires 4 nested loops, which is super slow in Python. In the code above, we've replaced the two inner loops with an element-by-element matrix multiplication for the kernel and the portion of the image applicable for the current indices into the convoution result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4ee529f5d4c6d34bc243bf284dac324",
     "grade": false,
     "grade_id": "cell-45f25cfd5055df25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise (15 points)\n",
    "\n",
    "The semi-naive implementation of the convolution function above could be sped up with the use of a fast low-level 2D convolution routine that makes the best possible use of the CPU's optimized instructions, pipelining of operations, etc. Take a look at [Laurent Perrinet's blog on 2D convolution implementations](https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html) and see how the two fastest implementations,\n",
    "scikit and numpy, outperform other methods and should vastly outperform the Python loop above. Reimplement the <code>convolve()</code> function above to be <code>convole2()</code> and compare\n",
    "the times taken by the naive and optimized versions of your convolution operation for the cat image. In your report, briefly describe the experiment and the\n",
    "results you obtained.\n",
    "\n",
    "- Implement an optimized CNN convolution operation (10 points)\n",
    "- Describe the experiment and the results (5 points)\n",
    "\n",
    "<details>\n",
    "    <summary><font size=\"3\" color=\"green\"><b>Get a hint!</b></font></summary>\n",
    "    \n",
    "Use <code>numpy.fft</code>'s `fft2` and `ifft2` functions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2468664036c707a2c2b11f254232e32c",
     "grade": false,
     "grade_id": "cell-47e5934a01b2de52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.fft import fft2, ifft2\n",
    "def convolve2(img, conv_filter):\n",
    "    output = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return output\n",
    "\n",
    "def conv2(img, conv_filters):\n",
    "    # Check shape of inputs\n",
    "    if len(img.shape) != len(conv_filters.shape) - 1: \n",
    "        raise Exception(\"Error: Number of dimensions in conv filter and image do not match.\")  \n",
    "\n",
    "    # Ensure filter depth is equal to number of channels in input\n",
    "    if len(img.shape) > 2 or len(conv_filters.shape) > 3:\n",
    "        if img.shape[-1] != conv_filters.shape[-1]:\n",
    "            raise Exception(\"Error: Number of channels in both image and filter must match.\")\n",
    "            \n",
    "    # Ensure filters are square\n",
    "    if conv_filters.shape[1] != conv_filters.shape[2]: \n",
    "        raise Exception('Error: Filter must be square (number of rows and columns must match).')\n",
    "\n",
    "    # Ensure filter dimensions are odd\n",
    "    if conv_filters.shape[1]%2==0: \n",
    "        raise Exception('Error: Filter must have an odd size (number of rows and columns must be odd).')\n",
    "\n",
    "    # Prepare output\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filters.shape[1]+1, \n",
    "                             img.shape[1]-conv_filters.shape[1]+1, \n",
    "                             conv_filters.shape[0]))\n",
    "\n",
    "    # Perform convolutions\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96a70ddd0272b7b2beae5e35a6dc5ff8",
     "grade": true,
     "grade_id": "cell-32f86bfd0d095bed",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "features = conv2(img, l1_filters)\n",
    "stop = datetime.datetime.now()\n",
    "%timeit conv2(img,l1_filters)\n",
    "\n",
    "c = stop - start\n",
    "elapsed = c.microseconds / 1000 # millisec\n",
    "\n",
    "print('Convolutional feature maps shape:', features.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(features[:,:,0], cmap='gray')\n",
    "ax2.imshow(features[:,:,1], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Test function: Do not remove\n",
    "assert elapsed < 200, \"Convolution is too slow, try again\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b15e8fedc9c6cdcdd8af0d38780e3154",
     "grade": true,
     "grade_id": "cell-a6f54bce0f162144",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling and relu\n",
    "\n",
    "Next, we consider the feedforward pooling and ReLU operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layer with particular size and stride\n",
    "\n",
    "def pooling(feature_map, size=2, stride=2):\n",
    "    pool_out = np.zeros((np.uint16((feature_map.shape[0]-size+1)/stride+1),\n",
    "                         np.uint16((feature_map.shape[1]-size+1)/stride+1),\n",
    "                         feature_map.shape[-1]))\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        r2 = 0\n",
    "        for r in np.arange(0,feature_map.shape[0]-size+1, stride):\n",
    "            c2 = 0\n",
    "            for c in np.arange(0, feature_map.shape[1]-size+1, stride):\n",
    "                pool_out[r2, c2, map_num] = np.max([feature_map[r:r+size,  c:c+size, map_num]])\n",
    "                c2 = c2 + 1\n",
    "            r2 = r2 +1\n",
    "    return pool_out\n",
    "\n",
    "# ReLU activation function\n",
    "\n",
    "def relu(feature_map):\n",
    "    relu_out = np.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in np.arange(0,feature_map.shape[0]):\n",
    "            for c in np.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the ReLU and pooling operations on the result of the previous convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relued_features = relu(features)\n",
    "pooled_features = pooling(relued_features)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "ax1.imshow(relued_features[:,:,0], cmap='gray')\n",
    "ax2.imshow(relued_features[:,:,1], cmap='gray')\n",
    "ax3.imshow(pooled_features[:,:,0], cmap='gray')\n",
    "ax4.imshow(pooled_features[:,:,1], cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize all of the feature maps in the model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First conv layer\n",
    "\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(\"conv layer 1...\")\n",
    "l1_feature_maps = conv(img, l1_filters)\n",
    "l1_feature_maps_relu = relu(l1_feature_maps)\n",
    "l1_feature_maps_relu_pool = pooling(l1_feature_maps_relu, 2, 2)\n",
    "\n",
    "# Second conv layer\n",
    "\n",
    "print(\"conv layer 2...\")\n",
    "l2_filters = np.random.rand(3, 5, 5, l1_feature_maps_relu_pool.shape[-1])\n",
    "l2_feature_maps = conv(l1_feature_maps_relu_pool, l2_filters)\n",
    "l2_feature_maps_relu = relu(l2_feature_maps)\n",
    "l2_feature_maps_relu_pool = pooling(l2_feature_maps_relu, 2, 2)\n",
    "#print(l2_feature_maps)\n",
    "\n",
    "# Third conv layer\n",
    "\n",
    "print(\"conv layer 3...\")\n",
    "l3_filters = np.random.rand(1, 7, 7, l2_feature_maps_relu_pool.shape[-1])\n",
    "l3_feature_maps = conv(l2_feature_maps_relu_pool, l3_filters)\n",
    "l3_feature_maps_relu = relu(l3_feature_maps)\n",
    "l3_feature_maps_relu_pool = pooling(l3_feature_maps_relu, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "\n",
    "fig0, ax0 = plt.subplots(nrows=1, ncols=1)\n",
    "ax0.imshow(img).set_cmap(\"gray\")\n",
    "ax0.set_title(\"Input Image\")\n",
    "ax0.get_xaxis().set_ticks([])\n",
    "ax0.get_yaxis().set_ticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "fig1, ax1 = plt.subplots(nrows=3, ncols=2)\n",
    "fig1.set_figheight(10)\n",
    "fig1.set_figwidth(10)\n",
    "ax1[0, 0].imshow(l1_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[0, 0].get_xaxis().set_ticks([])\n",
    "ax1[0, 0].get_yaxis().set_ticks([])\n",
    "ax1[0, 0].set_title(\"L1-Map1\")\n",
    "\n",
    "ax1[0, 1].imshow(l1_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[0, 1].get_xaxis().set_ticks([])\n",
    "ax1[0, 1].get_yaxis().set_ticks([])\n",
    "ax1[0, 1].set_title(\"L1-Map2\")\n",
    "\n",
    "ax1[1, 0].imshow(l1_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[1, 0].get_xaxis().set_ticks([])\n",
    "ax1[1, 0].get_yaxis().set_ticks([])\n",
    "ax1[1, 0].set_title(\"L1-Map1ReLU\")\n",
    "\n",
    "ax1[1, 1].imshow(l1_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[1, 1].get_xaxis().set_ticks([])\n",
    "ax1[1, 1].get_yaxis().set_ticks([])\n",
    "ax1[1, 1].set_title(\"L1-Map2ReLU\")\n",
    "\n",
    "ax1[2, 0].imshow(l1_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 0].set_title(\"L1-Map1ReLUPool\")\n",
    "\n",
    "ax1[2, 1].imshow(l1_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 1].set_title(\"L1-Map2ReLUPool\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "fig2, ax2 = plt.subplots(nrows=3, ncols=3)\n",
    "fig2.set_figheight(12)\n",
    "fig2.set_figwidth(12)\n",
    "ax2[0, 0].imshow(l2_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[0, 0].get_xaxis().set_ticks([])\n",
    "ax2[0, 0].get_yaxis().set_ticks([])\n",
    "ax2[0, 0].set_title(\"L2-Map1\")\n",
    "\n",
    "ax2[0, 1].imshow(l2_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[0, 1].get_xaxis().set_ticks([])\n",
    "ax2[0, 1].get_yaxis().set_ticks([])\n",
    "ax2[0, 1].set_title(\"L2-Map2\")\n",
    "\n",
    "ax2[0, 2].imshow(l2_feature_maps[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[0, 2].get_xaxis().set_ticks([])\n",
    "ax2[0, 2].get_yaxis().set_ticks([])\n",
    "ax2[0, 2].set_title(\"L2-Map3\")\n",
    "\n",
    "ax2[1, 0].imshow(l2_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[1, 0].get_xaxis().set_ticks([])\n",
    "ax2[1, 0].get_yaxis().set_ticks([])\n",
    "ax2[1, 0].set_title(\"L2-Map1ReLU\")\n",
    "\n",
    "ax2[1, 1].imshow(l2_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[1, 1].get_xaxis().set_ticks([])\n",
    "ax2[1, 1].get_yaxis().set_ticks([])\n",
    "ax2[1, 1].set_title(\"L2-Map2ReLU\")\n",
    "\n",
    "ax2[1, 2].imshow(l2_feature_maps_relu[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[1, 2].get_xaxis().set_ticks([])\n",
    "ax2[1, 2].get_yaxis().set_ticks([])\n",
    "ax2[1, 2].set_title(\"L2-Map3ReLU\")\n",
    "\n",
    "ax2[2, 0].imshow(l2_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[2, 0].get_xaxis().set_ticks([])\n",
    "ax2[2, 0].get_yaxis().set_ticks([])\n",
    "ax2[2, 0].set_title(\"L2-Map1ReLUPool\")\n",
    "\n",
    "ax2[2, 1].imshow(l2_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[2, 1].get_xaxis().set_ticks([])\n",
    "ax2[2, 1].get_yaxis().set_ticks([])\n",
    "ax2[2, 1].set_title(\"L2-Map2ReLUPool\")\n",
    "\n",
    "ax2[2, 2].imshow(l2_feature_maps_relu_pool[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[2, 2].get_xaxis().set_ticks([])\n",
    "ax2[2, 2].get_yaxis().set_ticks([])\n",
    "ax2[2, 2].set_title(\"L2-Map3ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=3)\n",
    "fig3.set_figheight(15)\n",
    "fig3.set_figwidth(15)\n",
    "ax3[0].imshow(l3_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[0].get_xaxis().set_ticks([])\n",
    "ax3[0].get_yaxis().set_ticks([])\n",
    "ax3[0].set_title(\"L3-Map1\")\n",
    "\n",
    "ax3[1].imshow(l3_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[1].get_xaxis().set_ticks([])\n",
    "ax3[1].get_yaxis().set_ticks([])\n",
    "ax3[1].set_title(\"L3-Map1ReLU\")\n",
    "\n",
    "ax3[2].imshow(l3_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[2].get_xaxis().set_ticks([])\n",
    "ax3[2].get_yaxis().set_ticks([])\n",
    "ax3[2].set_title(\"L3-Map1ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that at progressively higher layers of the network, we get coarser representations of the input. Since the filters at the later layers\n",
    "are random, they are not very structured, so we get a kind of blurring effect. These visualizations would be more meaningful in model with learned\n",
    "filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ed2237b02e688a2382a26f08ae9b33b",
     "grade": false,
     "grade_id": "cell-1fe35b7480741191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2 (15 points)\n",
    "\n",
    "Modify the three-layer CNN above to use your <code>conv2()</code> function. Check the result, explain what you did, and take note of any differences in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99b1629b86848db75761a565fbd2ce8d",
     "grade": true,
     "grade_id": "cell-c8b969e1fadbd215",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f26bea861a72b0d68997e1cac2b9741",
     "grade": true,
     "grade_id": "cell-19971aa19428b549",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in PyTorch\n",
    "\n",
    "Now that you've seen layered convolutions in action, at least in feed-forward mode, using structured kernels and random kernels, you can probably imagine how we would backpropagate loss through the ReLU, pooling, and convolutional layers. Don't worry, we won't make you implement it. Let's use the compute graph and gradient calculation features of PyTorch to do the work for us.\n",
    "\n",
    "We'll next do a more complete CNN example using PyTorch. We'll use the MNIST digits dataset again. The example\n",
    "is based on [Anand Saha's PyTorch tutorial](https://github.com/anandsaha/deep.learning.with.pytorch).\n",
    "\n",
    "As we learned last week, PyTorch has a few useful modules for us:\n",
    "1. cuda: GPU-based tensor computations\n",
    "2. nn: Neural network layer implementations and backpropagation via autograd\n",
    "3. torchvision: datasets, models, and image transformations for computer vision problems.\n",
    "\n",
    "The TorchVision module is a separate module that builds on PyTorch and provides datasets\n",
    "and operations useful for computer vision problems::\n",
    "1. datasets: TorchVision datasets are subclasses of `torch.utils.data.Dataset`. Some of the most commonly\n",
    "   used torchvision datasets available are MNIST, CIFAR, and COCO. In this example we will see how to load\n",
    "   the MNIST dataset using a custom subclass of the higher-level dataset class.\n",
    "2. transforms: Transforms are used for image transformations. These transformations are useful for preparing\n",
    "   images for input to our CNN and for so-called data augmentation, in which we perform randomized purturbations\n",
    "   of input images prior to each training iteration, giving us more variety in the training data. The\n",
    "   MNIST dataset from torchvision is in the PIL (Python Image Library) image format. To convert MNIST\n",
    "   images to tensors, we will use `transforms.ToTensor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# Set proxy in case it's not already set in our environment before loading torchvision\n",
    "\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# The functional module contains helper functions for defining neural network layers as simple functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST data\n",
    "\n",
    "First, let's load the data and apply a transfromation of the input elements (pixel intensities) assuming a specific mean and standard deviation over the entire training dataset. Note that PIL automatically normalizes the incoming pixel data, which are integers between 0 and 255, to the floating point range $[0..1]$. For this dataset, we'll use a mean of 0 and a standard deviation of 1, which together imply *no* normalization. We could change `mean` and `stddev` to apply a normalizing transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mean and standard deviation to use for normalization. For now, no transformation.\n",
    "\n",
    "mean = 0.0\n",
    "stddev = 1.0\n",
    "\n",
    "# Transform to apply to input images\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize([mean], [stddev])])\n",
    "\n",
    "# Datasets\n",
    "\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements of a torch `Dataset` can be accessed with array operations. The dataset has one array element for\n",
    "each input example. For supervised learning problems, each array element is usually a pair consisting of\n",
    "an input pattern and an output label (classification) or target value (regression). Let's grab one of the\n",
    "input images in the dataset. Note that a PyTorch tensor normally makes the feature map or image color channel the\n",
    "*first* dimension of the tensor, whereas it's usually the *last* dimension in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mnist_train[12][0].numpy()\n",
    "print('Input image 12 shape:', img.shape)\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used a mean of 0 and standard deviation of 1 in our normalizing transform, the image intensities still\n",
    "vary from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min pixel intensity:', img.min(), 'max pixel intensity:', img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to read one the labels, and also create a `DataLoader` for the dataset. A `DataLoader`\n",
    "goes through a `Dataset`, optionally shuffling the examples in the set, then loads examples in batches.\n",
    "The user of a loader applies an iterator to it to grab the batches one by one until the end of the dataset is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mnist_train[12][1]\n",
    "print('Label of image above:', label)\n",
    "\n",
    "# Reduce batch size if you get out-of-memory errors!\n",
    "\n",
    "batch_size = 1024\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NN model\n",
    "\n",
    "Now, let's use two convolutional layers followed by two fully connected layers. The input size of each image is (28,28,1). We will use a stride of 1 and padding of size 0 (valid convolution results only, which will shrink our\n",
    "input tensor).\n",
    "\n",
    "For the first convolutional layer, let's apply 20 filters of size $5\\times 5$.\n",
    "The output tensor size formula is $$\\text{output size} = \\frac{W - F + 2P}{S} + 1,$$\n",
    "where $W$ is the input size, $F$ is the filter size, $P$ is the padding size, and $S$ is the stride.\n",
    "\n",
    "In our case, we get $\\frac{(28,28,1) - (5,5,1) + (2*0)}{1} + 1$ for each filter, so for 10 filters we get\n",
    "an output tensor size of (24,24,10).\n",
    "\n",
    "The ReLU activation function is applied to the output of the first convolutional layer.\n",
    "\n",
    "For the second convolutional layer, we apply 20 filters of size (5,5), giving us output of size of (20,20,20). Maxpooling with a size of 2 is applied to the output of the second convolutional layer, thereby giving us an output size of of (10,10,20). The ReLU activation function is applied to the output of the maxpooling layer.\n",
    "\n",
    "Next we have two fully connected layers. The input of the first fully connected layer is flattened output of $10*10*20 = 2000$, with 50 nodes. The second layer is the output layer and has 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "               \n",
    "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
    "        self.relu1 = nn.ReLU()                            # Same as above\n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)     # 20 x 20 x 20  (after 2nd convolution)\n",
    "        #self.conv2_drop = nn.Dropout2d(p=0.5)            # Dropout is a regularization technqiue we discussed in class\n",
    "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 20  (after pooling)\n",
    "        self.relu2 = nn.ReLU()                            # Same as above \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(2000, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convolution Layer 1                    \n",
    "        x = self.conv1(x)                        \n",
    "        x = self.relu1(x)                        \n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        x = self.conv2(x)               \n",
    "        #x = self.conv2_drop(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Switch from activation maps to vectors\n",
    "        x = x.view(-1, 2000)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an instance of the neural network model, move it to the GPU, and set up our loss function\n",
    "and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "net = CNN_Model()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform 20 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    net.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss / iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct.item() / float(len(mnist_train_loader.dataset))))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    net.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss / iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar.item() / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is still learning something. We might want to train another 10 epochs or so to see if validation accuracy increases further. For now, though, we'll just save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(net.state_dict(), \"./3.model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_loss, label='training loss')\n",
    "plt.plot(valid_loss, label='validation loss')\n",
    "plt.title('MNIST loss, conv-5x5x10-relu-conv-5x5x20-maxpool-2x2-relu-fc50-fc10')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_accuracy, label='training accuracy')\n",
    "plt.plot(valid_accuracy, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from the loss and accuracy curves?\n",
    "1. We are not overfitting (at least not yet)\n",
    "2. We should continue training, as validation loss is still improving\n",
    "3. Validation accuracy is much higher than last week's fully connected models\n",
    "\n",
    "Now let's test on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 9\n",
    "img = mnist_valid[image_index][0].resize_((1, 1, 28, 28))\n",
    "img = Variable(img)\n",
    "label = mnist_valid[image_index][1]\n",
    "plt.imshow(img[0,0])\n",
    "net.eval()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    net = net.cpu()\n",
    "    img = img.cpu()\n",
    "    \n",
    "output = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(output.data, 1)\n",
    "print(\"Predicted label:\", predicted[0].item())\n",
    "print(\"Actual label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b3b86bd72e40c86ee4234e64d5e6724",
     "grade": false,
     "grade_id": "cell-5e05960a1a424836",
     "locked": true,
     "points": 70,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Take-home exercise (70 points)\n",
    "\n",
    "Apply the tech you've learned up till now to take Kaggle's 2013 [Dogs vs. Cats Challenge](https://www.kaggle.com/c/dogs-vs-cats). Download the training and test datasets and try to build the best PyTorch CNN you can for this dataset. Describe your efforts and the results in a brief lab report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
