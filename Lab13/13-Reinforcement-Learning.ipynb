{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Reinforcement Learning (RL)\n",
    "\n",
    "Today we'll have a gentle introduction to reinforcement learning.\n",
    "The material in today's lab comes from these references:\n",
    "\n",
    " - Pytorch 1.x Reinforcement Learning Cookbook (Packtpub)\n",
    " - Hands-On Reinforcement Learning for Games (Packtpub)\n",
    " - https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\n",
    " - Reinforcement Learning: An Introduction (Sutton et al.)\n",
    " - https://github.com/werner-duvaud/muzero-general (simulator code)\n",
    "\n",
    "\n",
    "## Reinforcement learning\n",
    "\n",
    "Reinforcement Learning (RL) is a machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback on its actions and experiences.\n",
    "RL uses rewards and punishment as signals for \"good\" and \"bad\" behavior.\n",
    "\n",
    "Generally, at each step, the agent outputs an action, which is input to the environment. The environment evolves according to its dynamics, the agent observes the new state of the environment\n",
    "and (optionally) a reward, and the process continues until hopefully the agent learns what behavior maximizes its reward.\n",
    "\n",
    "<img src=\"img/RL.jpg\" title=\"Introduction\" style=\"width: 600px;\" />\n",
    "\n",
    "\n",
    "## Markov decision process (MDP)\n",
    "\n",
    "A MDP is a discrete-time stochastic control process. The MDP model is based on the idea of an environment that evolves as a Markov chain.\n",
    "\n",
    "### Markov chain\n",
    "\n",
    "A Markov chain is a model of the dynamics of a discrete time system that obeys the (first order) \"Markov property,\" meaning that the state $s^{t+1}$ at time\n",
    "$t+1$ is conditionally independent of the state at times $0, \\ldots, t-1$ given the state at time $t$, i.e.,\n",
    "\n",
    "$$ p(s^{t+1} \\mid s^t, s^{t-1}, \\ldots, s^0) = p(s^{t+1} \\mid s^t). $$\n",
    "\n",
    "Informally, we might say that the current state is all you need to know to predict the next state.\n",
    "\n",
    "More precisely, a Markov chain is defined by a set of possible states $S={s_0, s_1, \\ldots, s_n}$ and a transition matrix $T(s,s')$\n",
    "containing the propbabilities of state $s$ transitioning to state $s'$. Here is a visualization of a simple Markov chain:\n",
    "\n",
    "<img src=\"img/RL_markov.png\" title=\"Markov chain\" style=\"width: 600px;\" />\n",
    "\n",
    "You might be interested in [this Markov chain simulator](https://setosa.io/ev/markov-chains/).\n",
    "\n",
    "Now, the dynamics of the environment in a MDP are slightly different from that of a simple Markov chain. We have to consider how the agent's\n",
    "actions affect the system's dynamics. At each time step, rather than just\n",
    "transitioning randomly to the next state, we add the agent's action as an external input or disturbance $a \\in A$, so (assuming a small number of discrete\n",
    "states and actions) the transition probabilities become a 3D tensor of size $|S|\\times |A|\\times |S|$\n",
    "mapping each state/action pair to a probability distribution over the states.\n",
    "\n",
    "### A simple MDP\n",
    "\n",
    "Suppose we have three states and two actions and that the state/action transition tensor is as follows:\n",
    "\n",
    "<img src=\"img/RL_statetrans.png\" title=\"\" style=\"width: 200px;\" />\n",
    "\n",
    "To complete our simple MDP, we need a *reward function* $R$ and a *discount factor* $\\gamma$. \n",
    "Suppose $R = [ 1, 0, -1 ]$ and $\\gamma = 0.5$. Let's define our MDP in Python with PyTorch tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# State transition function\n",
    "\n",
    "T = torch.tensor([[[0.8, 0.1, 0.1],\n",
    "                   [0.1, 0.6, 0.3]],\n",
    "                  [[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1]],\n",
    "                  [[0.6, 0.2, 0.2],\n",
    "                   [0.1, 0.4, 0.5]]])\n",
    "\n",
    "# Reward function\n",
    "\n",
    "R = torch.tensor([1.,0.,-1.])\n",
    "\n",
    "# Discount factor\n",
    "\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent's goal\n",
    "\n",
    "Once the MDP is defined, the agent's goal is to maximize its expected reward. If we start in state $s^0$ and perform a series\n",
    "of actions $a^0, a^1, \\ldots a^{T-1}$ placing us in state $s^1, s^2, \\ldots s^T$, we obtain the total reward\n",
    "\n",
    "$$ \\sum_{t=0}^T \\gamma^{t} R(s^t).$$\n",
    "\n",
    "The agent's goal is to behave so as to maximize the expected total reward. To do so,\n",
    "it should come up with a policy $\\pi : S \\times A \\rightarrow \\mathbb{R}$ giving a probability distribution\n",
    "over actions that can be executed in each state, then when in state $s$, sample action $a$ according to that\n",
    "distribution $\\pi(s,\\cdot)$, and repeat.\n",
    "\n",
    "Now the agent's goal can be clearly specified as finding an optimal policy\n",
    "\n",
    "$$ \\pi^* = \\textrm{argmax}_\\pi \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1})}\\left[ \\sum_{t=0}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "Under a particular policy $\\pi$, then, the *value* of state $s$ is the expected reward we obtain by following $\\pi$ from state $s$:\n",
    "\n",
    "$$ V^\\pi(s) = \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1}) \\mid s^0=s}\\left[ R(s) + \\sum_{t=1}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "The value function clearly obeys the *Bellman equations*\n",
    "\n",
    "$$ V^\\pi(s) = R(s) + \\gamma \\sum_{s',a'} \\pi(s,a') T(s,a',s') V^\\pi(s'). $$ \n",
    "\n",
    "### How good is a policy? Policy evaluation\n",
    "\n",
    "To determine how good a particular policy is, we use policy evaluation.\n",
    "Policy evaluation is an iterative algorithm. It starts with arbitrary values for each state\n",
    "and then iteratively updates the values based on the Bellman equations until the values converge.\n",
    "\n",
    "You can see this algorithm's pseudocode in Sutton's book on page 75.\n",
    "\n",
    "Here we compute the value of the three states in our MDP assuming the agent always peforms the first action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state, actions in enumerate(policy):\n",
    "            for action, action_prob in enumerate(actions):\n",
    "                V_temp[state] += action_prob * (rewards[state] + gamma * torch.dot(trans_matrix[state, action], V))\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "threshold = 0.0001\n",
    "\n",
    "policy_optimal = torch.tensor([[1.0, 0.0],\n",
    "                               [1.0, 0.0],\n",
    "                               [1.0, 0.0]])\n",
    "\n",
    "V = policy_evaluation(policy_optimal, T, R, gamma, threshold)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration\n",
    "\n",
    "Policy iteration starts with a random policy then uses policy evaluation and the resulting values to iteratively improve the policy until an optimal policy is obtained. It is, however,\n",
    "slow, due to the policy evaluation loop within the policy iteration loop.\n",
    "\n",
    "Here's an implementation using a slightly different formulation fo the reward as a function $R(s,a,s')$ of the current state $s$, action taken $a$, and resulting state $s'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            action = int(policy[state].item())\n",
    "            for new_state in range(n_state):\n",
    "                trans_prop = trans_matrix[state, action, new_state]\n",
    "                reward = rewards[state, action, new_state]\n",
    "                V_temp[state] += trans_prop * (reward + gamma * V[new_state])\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(trans_matrix, rewards, gamma):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    policy = torch.zeros(n_state)\n",
    "\n",
    "    for state in range(n_state):\n",
    "        v_actions = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for new_state in range(n_state):\n",
    "                trans_prop = trans_matrix[state, action, new_state]\n",
    "                reward = rewards[state, action, new_state]\n",
    "                v_actions[action] += trans_prop * (reward + gamma * V[new_state])\n",
    "        policy[state] = torch.argmax(v_actions)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    policy = torch.randint(high=n_action, size=(n_state,)).float()\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, trans_matrix, rewards, gamma, threshold)\n",
    "        policy_improved = policy_improvement(trans_matrix, rewards, gamma)\n",
    "        if torch.equal(policy_improved, policy):\n",
    "            return V, policy_improved\n",
    "        policy = policy_improved\n",
    "\n",
    "# Reward R(s,a,a') example\n",
    "\n",
    "R2 = torch.tensor([[[0.1,0.,-0.2],\n",
    "                    [0.2,0.,-0.1]],\n",
    "                   [[0.3,0.,-0.5],\n",
    "                    [0.1,0.,-0.2]],\n",
    "                   [[0.2,0.,-0.1],\n",
    "                    [1.,0.,-1.]]])\n",
    "\n",
    "V_optimal, optimal_policy = policy_iteration(T, R2, gamma, threshold)\n",
    "print(V_optimal)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "\n",
    "Value iteration is a much more efficient way to obtain the optimial policy. It calculates the value\n",
    "of each state on the assumption that the agent will deterministically select the action $a$ in state $s$\n",
    "that maximizes the expected reward after that. Once this value converges, the optimal policy is just\n",
    "to select the best action according to that value function:\n",
    "\n",
    "$$ V^*(s)=R(s) + \\max_a \\gamma \\sum_{s'} T(s,a,s') V^*(s') $$\n",
    "\n",
    "$$ \\pi^*(s) = \\mathrm{argmax}_a \\sum_{s'} T(s, a, s') V^*(s').$$\n",
    "\n",
    "Instead of taking the expectation (average) of values across all actions, we pick the action that achieves the maximal policy values.\n",
    "As above, we use the slightly different formulation in which the reward $R(s,a,s')$ is given based on initial state, action, and resulting state:\n",
    "\n",
    "$$ V^*(s) = \\max_a \\sum_{s'} T(s,a,s') [ R(s,a,s') + \\gamma V^*(s')]$$\n",
    "$$ \\pi^* = \\mathrm{argmax}_a \\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V^*(s')]$$\n",
    "\n",
    "Here's sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            v_actions = torch.zeros(n_action)\n",
    "            for action in range(n_action):\n",
    "                for new_state in range(n_state):\n",
    "                    trans_prop = trans_matrix[state, action, new_state]\n",
    "                    reward = rewards[state, action, new_state]\n",
    "                    v_actions[action] += trans_prop * (reward + gamma * V[new_state])\n",
    "            V_temp[state] = torch.max(v_actions)\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V = value_iteration(T, R2, gamma, threshold)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write some code now to find the optimal policy based on the values you got from value iteration above. Do you get the same result as for\n",
    "policy iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "Looking at policies in matrix form is a bit difficult.\n",
    "\n",
    "A popular simulation environment for RL is OpenAI Gym.\n",
    "\n",
    "[OpenAI](https://openai.com) is a research company trying to develop systems exhibiting *artificial general intelligence* (AGI).\n",
    "They developed Gym to support the development of RL algorithms. Gym\n",
    "provides many reinforcement learning simulations and tasks. Visit [the Gym website](https://gym.openai.com) for a full list of environments.\n",
    "\n",
    "<img src=\"img/RL_gym.PNG\" title=\"Gym example\" style=\"width: 600px;\" />\n",
    "\n",
    "### Installing Gym\n",
    "\n",
    "Let's install Gym. You should do this on your local machine, not within a remote Jupyter, so that you can\n",
    "see the interactive display that pops up. You can download this notebook and run it locally, or paste code\n",
    "into a Python IDE, or put it in a text file and run from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this locally, not on a remote Jupyter.\n",
    "\n",
    "!pip install gym\n",
    "\n",
    "# or try\n",
    "\n",
    "# !git clone https://github.com/openai/gym\n",
    "# !cd gym\n",
    "# pip install -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari games environment\n",
    "\n",
    "Atari games includes video games such as Alien, Pong, and Space Race. Here is an example of\n",
    "using the Space Invaders game. First, install the Atari simulation environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this locally, not on a remote Jupyter.\n",
    "\n",
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a Space Invaders environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# create environment\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "# reset environments\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you render the environment locally, \n",
    "you should get output like this:\n",
    "\n",
    "<img src=\"img/RL_SpaceInvaders.PNG\" title=\"Space Invaders\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rendering, you'll want to close the environment.\n",
    "\n",
    "<font size=\"3\" color=\"red\"><b>WARNING:</b></font>\n",
    "If you do work with Gym environments in Jupyter, you need to use <code><font color='red'>env.close()</font></code> after <code>env.render()</code>, or your program will get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "\n",
    "# import time\n",
    "# env.render()\n",
    "# time.sleep(5)\n",
    "#close the environment\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot use Atari games due to errors occurring in Windows, you can try another simulator such as CartPole (inverted pendulum):\n",
    "\n",
    "<img src=\"img/RL_CartPole.PNG\" title=\"Cart pole\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "\n",
    "#import gym\n",
    "#import time\n",
    "# create environment\n",
    "#env = gym.make('CartPole-v0')\n",
    "\n",
    "# reset environments\n",
    "#env.reset()\n",
    "\n",
    "# render the environment\n",
    "#env.render()\n",
    "#time.sleep(5)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Gym in Jupyter\n",
    "\n",
    "Before trying the example aboves, you might try these two solutions to running Gym in Jupyter notebooks:\n",
    "MP4 rendering and is real-time rendering. [More information is here](https://kyso.io/eoin/openai-gym-jupyter#code=both).\n",
    "\n",
    "#### Method 1: MP4 rendering\n",
    "\n",
    "Here we save the simulation as an MP4 and show it at the end.\n",
    "This method makes the simulation faster since there is no visualising happening every step. In this simulation we just make the space ship take a random move each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And show MP4 vdo by this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Show as GIF in realtime\n",
    "\n",
    "This method is useful for look at the simulation in realtime, but it does make the simulation take longer. It does not save the gif, but if you run this cell you will see the image change as the simulation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "plt.figure(figsize=(9,9))\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for _ in range(100):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this lab, I recommend you to use the way2 for smoother working ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the action space available, and try to get a sample from the action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "action = env.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute an action using `step()`.\n",
    "The `step` method returns the next state after the action is taken.\n",
    " - **new_state**: The new observation\n",
    " - **reward**: The reward associated with that action in that state.\n",
    " - **is_done**: A flag to tell the game end (True).\n",
    " - **info**: extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, is_done, info = env.step(action)\n",
    "print(is_done)\n",
    "print(reward)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a *while* loop with a random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "is_done = False\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "plt.figure(figsize=(9,9))\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "while not is_done:\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, is_done, info = env.step(action)\n",
    "    print(info)\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.03)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the environments to do reinforcement learning\n",
    "\n",
    "## Monte Carlo (MC) Method\n",
    "\n",
    "Monte Carlo method is a model-free which have no require any prior knowledge of the environment. MC method is more scalable than MDP. MC control is used for finding the optimal policy when a policy is not given. There are basically two types of MC control: on-policy and off-policy. On-policy method learns about the optimal policy by executing the policy and evaluating and improving it, while off-policy methods learn about the optimal policy using information gathered using another policy (usually $\\epsilon$-greedy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy Monte Carlo control\n",
    "\n",
    "On-policy Monte Carlo works look-a-like to policy iteration which has 2 phases: evaluation and improvement.\n",
    " - Evaluation phase: it evaluates the **action-values** (called **Q-function** $Q(s,a)$) instead of evaluates the value function.\n",
    " - Improvement phase: the policy is updated by assigning the optimal action to each stage: $\\pi(s)=argmax_a Q(s,a)$\n",
    "\n",
    "In this code below, we add **epsilon-greedy** policy which it will not exploit the best action all the time. The equations are:\n",
    " - Epsilon ($\\epsilon$):\n",
    "\n",
    "      $\\pi(s,a)=\\frac{\\epsilon}{|A|}$  When $|A|$ is the number of all possible actions.\n",
    " - Greedy:\n",
    " \n",
    "      $\\pi(s,a)=1-\\epsilon + \\frac{\\epsilon}{|A|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "def run_episode(env, Q, epsilon, n_action):\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    states = []\n",
    "    is_done = False\n",
    "    # without epsilon-greedy\n",
    "    # action = torch.randint(0, n_action, [1]).item()\n",
    "    ##################################################\n",
    "    while not is_done:\n",
    "        # with epsilon-greedy\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        best_action = torch.argmax(Q[state]).item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        #######################################################\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    return states, actions, rewards\n",
    "\n",
    "def mc_control_on_policy(env, gamma, n_episode, epsilon):\n",
    "    n_action = env.action_space.n\n",
    "    G_sum = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "    Q = defaultdict(lambda: torch.empty(env.action_space.n))\n",
    "    for episode in range(n_episode):\n",
    "        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)\n",
    "        return_t = 0\n",
    "        G = {}\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            G[(state_t, action_t)] = return_t\n",
    "            for state_action, return_t in G.items():\n",
    "                state, action = state_action\n",
    "                if state[0] <= 21:\n",
    "                    G_sum[state_action] += return_t\n",
    "                    N[state_action] += 1\n",
    "                    Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy\n",
    "\n",
    "gamma = 1\n",
    "n_episode = 500000\n",
    "epsilon = 0.1\n",
    "optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode, epsilon)\n",
    "# print(optimal_policy)\n",
    "# print(optimal_Q)\n",
    "\n",
    "def simulate_episode(env, policy):\n",
    "    state = env.reset()\n",
    "    is_done= False\n",
    "    while not is_done:\n",
    "        action = policy[state]\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        if is_done:\n",
    "            return reward\n",
    "\n",
    "n_episode = 100\n",
    "n_win_optimal = 0\n",
    "n_lose_optimal = 0\n",
    "for _ in range(n_episode):\n",
    "    reward = simulate_episode(env, optimal_policy)\n",
    "    if reward == 1:\n",
    "        n_win_optimal += 1\n",
    "    elif reward == -1:\n",
    "        n_lose_optimal += 1\n",
    "print('after episode 100, win ', n_win_optimal, ' lose ', n_lose_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Monte Carlo control\n",
    "\n",
    "The Off-policy method optimizes the **target policy** ($\\pi$) using data generated by another policy (**behavior policy** ($b$)).\n",
    " - Target policy: exploitation purposes, greedy with respect to its current Q-function.\n",
    " - Behavior policy: exploration purposes, generate behavior which the target policy used for learning. The behavior policy can be anything to confirm that it can explore all possibilities, then all actions and all states can be chosen with non-zero probabilities.\n",
    "\n",
    "The weight importand for state-action pair is calculated as:\n",
    "\n",
    "$w_t=\\sum_{k=t}[\\pi(a_k|s_k)/b(a_k|s_k)]$\n",
    " - $\\pi(a_k|s_k)$: probabilities of taking action $a_k$ in state $s_k$\n",
    " - $b(a_k|s_k)$: probabilities under the behavior policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "def gen_random_policy(n_action):\n",
    "    probs = torch.ones(n_action) / n_action\n",
    "    def policy_function(state):\n",
    "        return probs\n",
    "    return policy_function\n",
    "\n",
    "random_policy = gen_random_policy(env.action_space.n)\n",
    "\n",
    "def run_episode(env, behavior_policy):\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    states = []\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        probs = behavior_policy(state)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if is_done:\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "def mc_control_off_policy(env, gamma, n_episode, behavior_policy):\n",
    "    n_action = env.action_space.n\n",
    "    G_sum = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "    Q = defaultdict(lambda: torch.empty(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        W = {}\n",
    "        w = 1\n",
    "        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)\n",
    "        return_t = 0\n",
    "        G = {}\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            G[(state_t, action_t)] = return_t\n",
    "            w *= 1./ behavior_policy(state_t)[action_t]\n",
    "            W[(state_t, action_t)] = w\n",
    "            if action_t != torch.argmax(Q[state_t]).item():\n",
    "                break\n",
    "            \n",
    "        for state_action, return_t in G.items():\n",
    "            state, action = state_action\n",
    "            if state[0] <= 21:\n",
    "                G_sum[state_action] += return_t * W[state_action]\n",
    "                N[state_action] += 1\n",
    "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy\n",
    "\n",
    "gamma = 1\n",
    "n_episode = 500000\n",
    "optimal_Q, optimal_policy = mc_control_off_policy(env, gamma, n_episode, random_policy)\n",
    "# print(optimal_policy)\n",
    "# print(optimal_Q)\n",
    "\n",
    "def simulate_episode(env, policy):\n",
    "    state = env.reset()\n",
    "    is_done= False\n",
    "    while not is_done:\n",
    "        action = policy[state]\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        if is_done:\n",
    "            return reward\n",
    "\n",
    "n_episode = 100\n",
    "n_win_optimal = 0\n",
    "n_lose_optimal = 0\n",
    "for _ in range(n_episode):\n",
    "    reward = simulate_episode(env, optimal_policy)\n",
    "    if reward == 1:\n",
    "        n_win_optimal += 1\n",
    "    elif reward == -1:\n",
    "        n_lose_optimal += 1\n",
    "print('after episode 100, win ', n_win_optimal, ' lose ', n_lose_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference (TD)\n",
    "\n",
    "Temporal Difference (TD) learning is a model-free learning algorithm like MC learning. In MC learning, Q-function is called and updated at the end of the entire episode, but TD learning update Q-function every step of an episode. One of the TD learning algorithm is Q-learning\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "Q-leaning is an off-policy learning algorithm. The Q-function is based on the equation:\n",
    "\n",
    "$Q(s,a)=Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a')-Q(s,a))$\n",
    "\n",
    "Where\n",
    " - $\\alpha$: learning rate\n",
    " - $\\gamma$: discount factor\n",
    " - $\\max_{a'}Q(s',a')$: greedy behavior policy, the highest Q-value among those in state $s'$ is selected to generate learning data.\n",
    "\n",
    "We know from class that Q-learning finds the optimal greedy policy while running an epsilon-greedy policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "env_id = \"SpaceInvaders-v0\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# defining epsilon-greedy policy\n",
    "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
    "    def policy_function(state, Q, available_actions):\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        # print(probs)\n",
    "        # print(state)\n",
    "        # print(Q[state])\n",
    "        best_action = torch.argmax(Q[state]).item()\n",
    "        if not(best_action in available_actions):\n",
    "            best_action = -1\n",
    "            Q_max = -800000000\n",
    "            for i in range(n_action):\n",
    "                if i in available_actions and Q_max < Q[state][i]:\n",
    "                    Q_max = Q[state][i]\n",
    "                    best_action = i\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "    return policy_function\n",
    "\n",
    "def q_learning(env, gamma, n_episode, alpha, player):\n",
    "    \"\"\"\n",
    "    Obtain the optimal policy with off-policy Q-learning method\n",
    "    @param env: OpenAI Gym environment\n",
    "    @param gamma: discount factor\n",
    "    @param n_episode: number of episodes\n",
    "    @return: the optimal Q-function, and the optimal policy\n",
    "    \"\"\"\n",
    "    n_action = 9\n",
    "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
    "    print('start learning')\n",
    "    for episode in range(n_episode):\n",
    "        print(\"episode: \", episode + 1)\n",
    "        state = env.reset()\n",
    "        state = hash(tuple(state.reshape(-1)))\n",
    "\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = epsilon_greedy_policy(state, Q, available_actions)\n",
    "            next_state, reward, is_done, info = env.step(action)\n",
    "            next_state = hash(tuple(next_state.reshape(-1)))\n",
    "            td_delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "\n",
    "            length_episode[episode] += 1\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "\n",
    "# the more episodes have been learned, the cleverer will be get.\n",
    "n_episode = 10\n",
    "\n",
    "alpha = 0.4\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "available_actions = numpy.arange(env.action_space.n)\n",
    "available_actions = torch.from_numpy(available_actions)\n",
    "epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)\n",
    "\n",
    "length_episode = [0] * n_episode\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "# agent play first\n",
    "optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha, 1)\n",
    "\n",
    "\n",
    "print('The optimal policy:\\n', optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(length_episode)\n",
    "plt.title('Episode length over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(total_reward_episode)\n",
    "print(total_reward_episode[-100:])\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def play_game(Q, available_actions):\n",
    "    done = False\n",
    "    plt.figure(figsize=(9,9))\n",
    "    img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "    state = env.reset()\n",
    "    state = hash(tuple(state.reshape(-1)))\n",
    "    while(not done):\n",
    "        action = epsilon_greedy_policy(state, Q, available_actions)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # env.render() No use!\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.03)\n",
    "        state = next_state\n",
    "        state = hash(tuple(state.reshape(-1)))\n",
    "\n",
    "play_game(optimal_Q, available_actions)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "A deep Q-Network (DQN) is similar to a supervised regression model $F_{\\theta}$, but it more specifically maps states to action values directly instead of using a set of features.\n",
    "\n",
    "A DQN is trained to output $Q(s,a)$ values for each action given the input state $s$. In operation, in state $s$, the action $a$ is chosen greedily based on $Q(s,a)$ or stochastically\n",
    "following an epsilon-greedy policy.\n",
    "\n",
    "<img src=\"img/RL2_DQN.png\" title=\"DQN\" style=\"width: 800px;\" />\n",
    "\n",
    "In tabular Q learning, the update rule is an off-policy TD learning rule. When we take\n",
    "action $a$ in state $s$ receiving reward $r$, we update $Q(s,a)$ as\n",
    "\n",
    "$$Q(s,a)=Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)),$$\n",
    "\n",
    "where\n",
    " - $s'$ is the resulting state after taking action $a$ in state $s$\n",
    " - $\\max_{a'}Q(s',a')$ is value of the action $a'$ we would take in state $s'$ according to a greedy behavior policy.\n",
    "\n",
    "A DQN does the same thing using backpropagation, minimizing inconsistencies in its $Q$ estimates. At each step, the difference\n",
    "between the estimated value and the observed data from the subsequent step should be minimized, giving us a kind of regression\n",
    "problem, for which a squared error loss function is appopriate, giving us a delta for the $a$th output of\n",
    "\n",
    "$$\\delta_a=r+\\gamma\\max_{a'}Q(s')_{a'}-Q(s)_{a}.$$\n",
    "\n",
    "With an appropriate exploration strategy and learning rate, DQN should find the optimal network model best approximating\n",
    "the state-value function $Q(s,a)$ for each possible state and action.\n",
    "\n",
    "## DQN Example: Cartpole\n",
    "\n",
    "Let's develop a sample DQN application step by step. First, some imports we'll need.\n",
    "We'll use the `gym` package like last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "\n",
    "# Select GPU or CPU as device\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$ decay schedule\n",
    "\n",
    "Recall that some of the theoretical results on TD learning assume $\\epsilon$-greedy exploration with $\\epsilon$ decaying slowly to 0\n",
    "over time. Let's define an exponential decay schedule for $\\epsilon$. First, an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "# Define epsilon as a function of time (episode index)\n",
    "\n",
    "eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "\n",
    "# Note that the above lambda expression is equivalent to explicitly defining a function:\n",
    "# def epsilon_episode(episode):\n",
    "#     return epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "\n",
    "plt.plot([eps_by_episode(i) for i in range(10000)])\n",
    "plt.title('Epsilon as function of time')\n",
    "plt.xlabel('Time (episode index)')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a reusable function to generate an annealing schedule function according to given parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon annealing schedule generator\n",
    "\n",
    "def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n",
    "    eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "    return eps_by_episode\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "\n",
    "We know that deep learning methods learn faster when training samples are combined into batches. This speeds up learning and also makes it more stable by averaging\n",
    "updates over multiple samaples.\n",
    "\n",
    "RL algorithms also benefit from batched training. However, we see that the standard Q learning rule always updates $Q$ estimates using the most recent experience.\n",
    "If we always trained on batches consisting of samples of the most recent behavior, correlations between successive state action pairs will make learning less effective.\n",
    "So we would also like to select random training samples to make them look more like the i.i.d. sampling that supervised learning performs well under.\n",
    "\n",
    "In RL, the standard way of doing this is to create a large buffer of past state action pairs then form training batches by sampling from that replay buffer.\n",
    "Our replay buffer will store tuples consisting of an observed state, an action, the next_state, the reward, and the termination signal obtained by the agent at that point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Add batch index dimension to state representations\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic DQN\n",
    "\n",
    "Next, a basic DQN class. We just create a neural network that takes as input a state\n",
    "and returns an output vector indiciating the value of each possible action $Q(s,a)$.\n",
    "\n",
    "The steps we take during learning will be as follows:\n",
    "\n",
    "<img src=\"img/RL2_DQNstep.jpeg\" title=\"\" style=\"width: 600px;\" />\n",
    "\n",
    "To implement the policy, besides the usual `forward()` method, we add one additional method `act()`,\n",
    "which samples an $\\epsilon$-greedy action for state $s$ using the current estimate $Q(s,a)$.\n",
    "`act()` will be used to implement step 1 in the pseudocode above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_state, n_action):\n",
    "        super(DQN, self).__init__()        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_state, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_action)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # Get an epsilon greedy action for given state\n",
    "        if random.random() > epsilon: # Use argmax_a Q(s,a)\n",
    "            state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gym environment, prepare DQN for training\n",
    "\n",
    "Next we set up a gym environment for the cartpole simulation, create a DQN model with Adam optimization, and create a replay buffer of length 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "In the training step, we sample a batch from the replay buffer, calculate $Q(s,a)$ and $\\max_{a'} Q(s',a')$, calculate\n",
    "the target $Q$ value $r + \\gamma\\max_{a'}Q(s',a')$, the mean squared loss between the predicted and target $Q$ values,\n",
    "and then backpropagate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(model, batch_size, gamma=0.99):\n",
    "\n",
    "    # Get batch from replay buffer\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors. Creating Variables is not necessary with more recent PyTorch versions.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "\n",
    "    # Calculate Q(s) and Q(s')\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    # Get Q(s,a) and max_a' Q(s',a')\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    # Calculate target for Q(s,a): r + gamma max_a' Q(s',a')\n",
    "    # Note that the done signal is used to terminate recursion at end of episode.\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # Calculate MSE loss. Variables are not needed in recent PyTorch versions.\n",
    "    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot rewards and losses\n",
    "\n",
    "Here's a little function to plot relevant details for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(episode, rewards, losses):\n",
    "    # clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (episode, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)   \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The training loop lets the agent play the game until the end of the episode.\n",
    "Each step is appended to the replay buffer. We don't do any learning until the buffer's length reaches the batch_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "\n",
    "    # Get initial state input\n",
    "    state = env.reset()\n",
    "\n",
    "    # Execute episodes iterations\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # Get initial epsilon greedy action\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = model.act(state, epsilon)\n",
    "        \n",
    "        # Take a step\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Append experience to replay buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Start a new episode if done signal is received\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        # Train on a batch if we've got enough experience\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(model, batch_size, gamma)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return model,all_rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!\n",
    "\n",
    "Let's train our DQN model for 10,000 steps in the cartpole simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,all_rewards, losses = train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play in the simulation\n",
    "\n",
    "You can run your simulation in Jupyter if you have OpenGL installed, but after the simulation is finished, you must close the\n",
    "simulator with `env.close()` (this closes the simulator, not the environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def play_game(model):\n",
    "    done = False\n",
    "    plt.figure(figsize=(9,9))\n",
    "    img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "    state = env.reset()\n",
    "    while(not done):\n",
    "        action = model.act(state, epsilon_final)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # env.render() No use!\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.03)\n",
    "        state = next_state\n",
    "\n",
    "play_game(model)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42b8b4cc4d9375b7d90dc3a1f99439dc",
     "grade": false,
     "grade_id": "cell-2f62f416f34d5296",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### In class exercise (50 points)\n",
    "\n",
    "Apply DQN to the CarRacing-v0 environment.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69986ddda2932e77834d7e725088366a",
     "grade": false,
     "grade_id": "cell-71f183bf97643dd9",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Take home exercise (50 points)\n",
    "\n",
    "Apply DQN to Space Invaders. How well can you score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
